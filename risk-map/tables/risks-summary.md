| ID   | Title                                  | Description                                                                                                                                                                                                                           | Category                         |
|:-----|:---------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------|
| ADI  | Adapter/PEFT Injection                 | Trojaned adapters merged at runtime to bypass safety or exfiltrate data. Adapter/PEFT Injection can compromise model behavior and enable unauthorized access to sensitive information or system resources.<br>                        | risksDeploymentAndInfrastructure |
| ASC  | Accelerator Side-channels              | Cross-tenant leakage via GPU/TPU timing, cache, and memory side-channels. Accelerator Side-channels can compromise data confidentiality and enable unauthorized access to sensitive information in shared computing environments.<br> | risksDeploymentAndInfrastructure |
| COV  | Covert Channels in Model Outputs       | Hidden information transmission through model outputs or behavior patterns. Covert Channels in Model Outputs can enable unauthorized data exfiltration and steganographic communication bypassing security controls.<br>              | risksRuntimeOutputSecurity       |
| DMS  | Denial of ML Service                   | Overloading ML systems with resource-intensive queries. Like traditional DoS attacks, Denial of ML Service can reduce availability of or entirely disrupt a service.<br>                                                              | risksRuntimeInputSecurity        |
| DP   | Data Poisoning                         | Altering data sources used to train the model. In terms of impact, Data Poisoning is comparable to modifying the logic of an application to change its behavior.<br>                                                                  | risksSupplyChainAndDevelopment   |
| EBM  | Evaluation/Benchmark Manipulation      | Poisoned or leaked evaluation sets misleading safety and robustness signals. Evaluation/Benchmark Manipulation can compromise model assessment accuracy and lead to deployment of unsafe or unreliable AI systems.<br>                | risksRuntimeDataSecurity         |
| EDH  | Excessive Data Handling                | Unauthorized collection, retention, processing, or sharing of user data. Excessive Data Handling may lead to policy and legal challenges.<br>                                                                                         | risksSupplyChainAndDevelopment   |
| EDW  | Economic Denial of Wallet              | Cost abuse via token inflation, long context, or tool loops that spike spend. Economic Denial of Wallet can lead to unexpected financial losses and service disruption through resource exhaustion attacks.<br>                       | risksRuntimeInputSecurity        |
| FLP  | Federated/Distributed Training Privacy | Gradient leakage and inversion attacks from untrusted clients in federated learning. Federated/Distributed Training Privacy risks can expose sensitive training data and compromise participant privacy.<br>                          | risksSupplyChainAndDevelopment   |
| IIC  | Insecure Integrated Component          | Software vulnerabilities that can be leveraged to compromise AI models. Insecure Integrated Component can lead to privacy and security concerns, as well as potential ethical and legal challenges.<br>                               | risksDeploymentAndInfrastructure |
| IMO  | Insecure Model Output                  | Unvalidated model output passed to the end user. Insecure Model Output poses risks to organizational reputation, security, and user safety.<br>                                                                                       | risksRuntimeOutputSecurity       |
| ISD  | Inferred Sensitive Data                | Model inferring personal information not contained in training data or inputs. Inferred Sensitive Data may be considered a data privacy incident.<br>                                                                                 | risksRuntimeDataSecurity         |
| MDT  | Model Deployment Tampering             | Unauthorized changes to model deployment components. Model Deployment Tampering can result in changes to model behavior.<br>                                                                                                          | risksDeploymentAndInfrastructure |
| MEV  | Model Evasion                          | Changes to a prompt input to cause the model to produce incorrect inferences. Model Evasion can lead to reputational, legal, security, and privacy risks.<br>                                                                         | risksRuntimeInputSecurity        |
| MLD  | Malicious Loader/Deserialization       | Unsafe loaders for models and tokenizers that can cause remote code execution or integrity compromise. Malicious Loader/Deserialization poses significant security risks including system compromise and data breaches.<br>           | risksSupplyChainAndDevelopment   |
| MRE  | Model Reverse Engineering              | Recreating a model by analyzing its inputs, outputs, and behaviors. A reverse engineer model can be used to create imitation products or adversarial attacks.<br>                                                                     | risksDeploymentAndInfrastructure |
| MST  | Model Source Tampering                 | Tampering with the model's code or data. Model Source Tampering is similar to tampering with traditional software code, and can create vulnerabilities or unintended behavior.<br>                                                    | risksSupplyChainAndDevelopment   |
| MXF  | Model Exfiltration                     | Theft of a model. Similar to stealing code, this threat has both intellectual property and security implications.<br>                                                                                                                 | risksDeploymentAndInfrastructure |
| ORH  | Orchestrator/Route Hijack              | Silent model or route swaps via configuration tampering or prompt-based routing abuse. Orchestrator/Route Hijack can redirect requests to malicious models or compromise routing integrity in AI systems.<br>                         | risksRuntimeOutputSecurity       |
| PCP  | Prompt/Response Cache Poisoning        | Cross-user contamination via shared LLM caches lacking isolation and validation. Prompt/Response Cache Poisoning can lead to information leakage, misinformation propagation, and unauthorized access to cached content.<br>          | risksRuntimeDataSecurity         |
| PIJ  | Prompt Injection                       | Tricking a model to run unintended commands. In terms of impact, Prompt Injection can change a model's behavior.<br>                                                                                                                  | risksRuntimeInputSecurity        |
| RA   | Rogue Actions                          | Unintentional model-based actions executed via extensions. Rogue Actions can create a cascading, risk to organizational reputation, user trust, security, and safety.<br>                                                             | risksRuntimeOutputSecurity       |
| RVP  | Retrieval/Vector Store Poisoning       | Poisoning retrieval corpora or vector indices to steer RAG outputs. Retrieval/Vector Store Poisoning can compromise the integrity of knowledge retrieval systems and lead to misinformation or malicious content injection.<br>       | risksRuntimeOutputSecurity       |
| SDD  | Sensitive Data Disclosure              | Disclosure of sensitive data by the model. Sensitive Data Disclosure poses a threat to user privacy, organizational reputation, and intellectual property.<br>                                                                        | risksRuntimeDataSecurity         |
| UTD  | Unauthorized Training Data             | Using unauthorized data for model training. Using a model trained with Unauthorized Training Data might lead to legal or ethical challenges.<br>                                                                                      | risksSupplyChainAndDevelopment   |